
@misc{nguyen,
      title={SARAH: A Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient}, 
      author={Lam M. Nguyen and Jie Liu and Katya Scheinberg and Martin Takáč},
      year={2017},
      eprint={1703.00102},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{kingma,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Beznosikov,
   title={Random-reshuffled SARAH does not need full gradient computations},
   volume={18},
   ISSN={1862-4480},
   url={http://dx.doi.org/10.1007/s11590-023-02081-x},
   DOI={10.1007/s11590-023-02081-x},
   number={3},
   journal={Optimization Letters},
   publisher={Springer Science and Business Media LLC},
   author={Beznosikov, Aleksandr and Takáč, Martin},
   year={2023},
   month=dec, pages={727–749} }


@article{Chauhan,
   title={SAAGs: Biased stochastic variance reduction methods for large-scale learning},
   volume={49},
   ISSN={1573-7497},
   url={http://dx.doi.org/10.1007/s10489-019-01450-3},
   DOI={10.1007/s10489-019-01450-3},
   number={9},
   journal={Applied Intelligence},
   publisher={Springer Science and Business Media LLC},
   author={Chauhan, Vinod Kumar and Sharma, Anuj and Dahiya, Kalpana},
   year={2019},
   month=apr, pages={3331–3361} }


@article{Grbzbalaban,
   title={Why random reshuffling beats stochastic gradient descent},
   volume={186},
   ISSN={1436-4646},
   url={http://dx.doi.org/10.1007/s10107-019-01440-w},
   DOI={10.1007/s10107-019-01440-w},
   number={1–2},
   journal={Mathematical Programming},
   publisher={Springer Science and Business Media LLC},
   author={Gürbüzbalaban, M. and Ozdaglar, A. and Parrilo, P. A.},
   year={2019},
   month=oct, pages={49–84} }

@misc{chenadamenhanced,
      title={An Adam-enhanced Particle Swarm Optimizer for Latent Factor Analysis}, 
      author={Jia Chen and Renyu Zhang and Yuanyi Liu},
      year={2023},
      eprint={2302.11956},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wangprovable,
      title={Provable Adaptivity in Adam}, 
      author={Bohan Wang and Yushun Zhang and Huishuai Zhang and Qi Meng and Zhi-Ming Ma and Tie-Yan Liu and Wei Chen},
      year={2022},
      eprint={2208.09900},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{defossezsimple,
      title={A Simple Convergence Proof of Adam and Adagrad}, 
      author={Alexandre Défossez and Léon Bottou and Francis Bach and Nicolas Usunier},
      year={2022},
      eprint={2003.02395},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{inproceedings,
author = {Bock, Sebastian and Goppold, Josef and Weiß, Martin},
year = {2018},
month = {04},
pages = {},
title = {An improvement of the convergence proof of the ADAM-Optimizer}
}

@misc{heconvergence,
      title={Convergence of Adam for Non-convex Objectives: Relaxed Hyperparameters and Non-ergodic Case}, 
      author={Meixuan He and Yuqing Liang and Jinlan Liu and Dongpo Xu},
      year={2023},
      eprint={2307.11782},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@article{adamrandomblock,
author = {Liu, Miaomiao and Yao, Dan and Liu, Zhigang and Guo, Jingfeng and Chen, Jing},
year = {2023},
month = {01},
pages = {1-14},
title = {An Improved Adam Optimization Algorithm Combining Adaptive Coefficients and Composite Gradients Based on Randomized Block Coordinate Descent},
volume = {2023},
journal = {Computational Intelligence and Neuroscience},
doi = {10.1155/2023/4765891}
}

@misc{ruderoverview,
      title={An overview of gradient descent optimization algorithms}, 
      author={Sebastian Ruder},
      year={2017},
      eprint={1609.04747},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{tensorflow,
  title = {{TensorFlow Keras Optimizers Method} tf.keras.optimizers},
  howpublished = {\url{https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam}},
  note = {Accessed: 2024-04-20}
}

@article{sarahm,
title = {SARAH-M: A fast stochastic recursive gradient descent algorithm via momentum},
journal = {Expert Systems with Applications},
volume = {238},
pages = {122295},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122295},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423027975},
author = {Zhuang Yang},
}

@misc{reddidivergence,
      title={On the Convergence of Adam and Beyond}, 
      author={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},
      year={2019},
      eprint={1904.09237},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{winequality,
  author = {Cortez,Paulo and Cerdeira,A. and Almeida,F. and Matos,T.  and Reis,J.},
  title = {{Wine Quality}},
  year = {2009},
  howpublished = {UCI Machine Learning Repository},
  note = {{DOI}: https://doi.org/10.24432/C56S3T}
}

