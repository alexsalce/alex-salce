
@misc{nguyen,
      title={SARAH: A Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient}, 
      author={Lam M. Nguyen and Jie Liu and Katya Scheinberg and Martin Takáč},
      year={2017},
      eprint={1703.00102},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1703.00102}, 
}

@misc{kingma,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Beznosikov,
   title={Random-reshuffled SARAH does not need full gradient computations},
   volume={18},
   ISSN={1862-4480},
   url={http://dx.doi.org/10.1007/s11590-023-02081-x},
   DOI={10.1007/s11590-023-02081-x},
   number={3},
   journal={Optimization Letters},
   publisher={Springer Science and Business Media LLC},
   author={Beznosikov, Aleksandr and Takáč, Martin},
   year={2023},
   month=dec, pages={727–749} }


@article{Chauhan,
   title={SAAGs: Biased stochastic variance reduction methods for large-scale learning},
   volume={49},
   ISSN={1573-7497},
   url={http://dx.doi.org/10.1007/s10489-019-01450-3},
   DOI={10.1007/s10489-019-01450-3},
   number={9},
   journal={Applied Intelligence},
   publisher={Springer Science and Business Media LLC},
   author={Chauhan, Vinod Kumar and Sharma, Anuj and Dahiya, Kalpana},
   year={2019},
   month=apr, pages={3331–3361} }


@article{Grbzbalaban,
   title={Why random reshuffling beats stochastic gradient descent},
   volume={186},
   ISSN={1436-4646},
   url={http://dx.doi.org/10.1007/s10107-019-01440-w},
   DOI={10.1007/s10107-019-01440-w},
   number={1–2},
   journal={Mathematical Programming},
   publisher={Springer Science and Business Media LLC},
   author={Gürbüzbalaban, M. and Ozdaglar, A. and Parrilo, P. A.},
   year={2019},
   month=oct, pages={49–84} }

@misc{chenadamenhanced,
      title={An Adam-enhanced Particle Swarm Optimizer for Latent Factor Analysis}, 
      author={Jia Chen and Renyu Zhang and Yuanyi Liu},
      year={2023},
      eprint={2302.11956},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wangprovable,
      title={Provable Adaptivity in Adam}, 
      author={Bohan Wang and Yushun Zhang and Huishuai Zhang and Qi Meng and Zhi-Ming Ma and Tie-Yan Liu and Wei Chen},
      year={2022},
      eprint={2208.09900},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{defossezsimple,
      title={A Simple Convergence Proof of Adam and Adagrad}, 
      author={Alexandre Défossez and Léon Bottou and Francis Bach and Nicolas Usunier},
      year={2022},
      eprint={2003.02395},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{inproceedings,
author = {Bock, Sebastian and Goppold, Josef and Weiß, Martin},
year = {2018},
month = {04},
pages = {},
title = {An improvement of the convergence proof of the ADAM-Optimizer}
}

@misc{heconvergence,
      title={Convergence of Adam for Non-convex Objectives: Relaxed Hyperparameters and Non-ergodic Case}, 
      author={Meixuan He and Yuqing Liang and Jinlan Liu and Dongpo Xu},
      year={2023},
      eprint={2307.11782},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@article{adamrandomblock,
author = {Liu, Miaomiao and Yao, Dan and Liu, Zhigang and Guo, Jingfeng and Chen, Jing},
year = {2023},
month = {01},
pages = {1-14},
title = {An Improved Adam Optimization Algorithm Combining Adaptive Coefficients and Composite Gradients Based on Randomized Block Coordinate Descent},
volume = {2023},
journal = {Computational Intelligence and Neuroscience},
doi = {10.1155/2023/4765891}
}

@misc{ruderoverview,
      title={An overview of gradient descent optimization algorithms}, 
      author={Sebastian Ruder},
      year={2017},
      eprint={1609.04747},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{tensorflow,
  title = {{TensorFlow Keras Optimizers Method} tf.keras.optimizers},
  howpublished = {\url{https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam}},
  note = {Accessed: 2024-04-20}
}

@article{sarahm,
title = {SARAH-M: A fast stochastic recursive gradient descent algorithm via momentum},
journal = {Expert Systems with Applications},
volume = {238},
pages = {122295},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122295},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423027975},
author = {Zhuang Yang},
}

@misc{reddidivergence,
      title={On the Convergence of Adam and Beyond}, 
      author={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},
      year={2019},
      eprint={1904.09237},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{winequality,
  author = {Cortez,Paulo and Cerdeira,A. and Almeida,F. and Matos,T.  and Reis,J.},
  title = {{Wine Quality}},
  year = {2009},
  howpublished = {UCI Machine Learning Repository},
  note = {{DOI}: https://doi.org/10.24432/C56S3T}
}

@inproceedings{Johnson,
author = {Johnson, Rie and Zhang, Tong},
title = {Accelerating stochastic gradient descent using predictive variance reduction},
year = {2013},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Stochastic gradient descent is popular for large scale optimization but has slow convergence asymptotically due to the inherent variance. To remedy this problem, we introduce an explicit variance reduction method for stochastic gradient descent which we call stochastic variance reduced gradient (SVRG). For smooth and strongly convex functions, we prove that this method enjoys the same fast convergence rate as those of stochastic dual coordinate ascent (SDCA) and Stochastic Average Gradient (SAG). However, our analysis is significantly simpler and more intuitive. Moreover, unlike SDCA or SAG, our method does not require the storage of gradients, and thus is more easily applicable to complex problems such as some structured prediction problems and neural network learning.},
booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 1},
pages = {315–323},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'13}
}

@misc{arnold2019,
      title={Reducing the variance in online optimization by transporting past gradients}, 
      author={Sébastien M. R. Arnold and Pierre-Antoine Manzagol and Reza Babanezhad and Ioannis Mitliagkas and Nicolas Le Roux},
      year={2019},
      eprint={1906.03532},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1906.03532}, 
}

